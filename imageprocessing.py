# -*- coding: utf-8 -*-
"""ImageProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-xoONCLdpH8h76WJz-P43WeB8pd1vwZ

Step 0: Setup Workspace
"""

from google.colab import drive
import os
import json

# Mount Drive
drive.mount('/content/drive')

WORK_DIR = '/content/drive/MyDrive/Amazon_ML_Challenge/student_resource'
DATA_DIR = os.path.join(WORK_DIR, 'dataset')
IMG_DIR = os.path.join(WORK_DIR, 'images')
EMB_DIR = os.path.join(WORK_DIR, 'embeddings')
MODEL_DIR = os.path.join(WORK_DIR, 'models')
LOG_FILE = os.path.join(WORK_DIR, 'progress.json')

# Create directories
os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(EMB_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# Load or initialize progress log
if os.path.exists(LOG_FILE):
    with open(LOG_FILE, 'r') as f:
        progress = json.load(f)
else:
    progress = {
        "downloaded_batches": [],
        "embedded_batches": [],
        "trained_epochs": 0
    }

print("Workspace ready!")

"""Step 1: Download Images and compressing"""

import os
import json
import pandas as pd
from PIL import Image
import requests
from io import BytesIO
from tqdm import tqdm
import glob
from pathlib import Path

# Load train CSV
train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))
train = train[['sample_id', 'image_link', 'price']]

# üîç Collect all already existing images (case-insensitive, any extension)
existing_files = {p.stem.lower() for p in Path(IMG_DIR).iterdir() if p.is_file()}


def download_and_compress(url, save_path, target_size=(224,224), quality=75):
    try:
        r = requests.get(url, timeout=10)
        if r.status_code != 200:
            return False
        img = Image.open(BytesIO(r.content)).convert('RGB')
        img = img.resize(target_size)
        img.save(save_path, format='JPEG', quality=quality)
        return True
    except:
        return False

# ‚úÖ Quick Skip if almost all images exist
if len(existing_files) >= len(train) - 5:  # allow up to 5 missing
    print(f"üéâ All images already present ({len(existing_files)}/{len(train)}) ‚Äî skipping download check!")
else:
    missing = []
    success = 0
    failed = 0

    pbar = tqdm(train.itertuples(), total=len(train), desc="Checking & downloading missing images")

    for row in pbar:
        img_name = str(row.sample_id).lower()
        if img_name not in existing_files:  # ‚úÖ Skip if already exists
            img_path = os.path.join(IMG_DIR, f"{row.sample_id}.jpg")
            ok = download_and_compress(row.image_link, img_path)
            if ok:
                success += 1
                existing_files.add(img_name)
            else:
                failed += 1
                missing.append((row.sample_id, row.image_link))

    pbar.close()
    print(f"\n‚úÖ Newly downloaded: {success}")
    print(f"‚ö†Ô∏è Failed downloads: {failed}")

    # Save list of failed ones for manual retry later
    if missing:
        failed_df = pd.DataFrame(missing, columns=["sample_id", "image_link"])
        failed_df.to_csv(os.path.join(DATA_DIR, "failed_downloads.csv"), index=False)
        print(f"‚ùå Saved missing images list to failed_downloads.csv")
    else:
        print("üéâ All images are now downloaded!")

"""Step 2: Extract Embeddings"""

!pip install transformers --quiet
import os
import json
import numpy as np
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm

# ------------------- #
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ------------------- #
# GPU batch size auto
def get_auto_batch_size(max_mem_gb=12, approx_image_mb=0.05):
    free_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    batch_size = int(min(max_mem_gb / approx_image_mb, free_mem_gb / approx_image_mb))
    return max(1, min(batch_size, 128))  # clamp 1‚Äì128

BATCH_SIZE_GPU = get_auto_batch_size()
print(f"Using GPU batch size: {BATCH_SIZE_GPU}")

# ------------------- #
# Embedding helper
def get_embeddings_batch(img_paths):
    imgs, valid_paths = [], []
    for p in img_paths:
        try:
            imgs.append(Image.open(p).convert("RGB"))
            valid_paths.append(p)
        except:
            print(f"‚ö†Ô∏è Skipped corrupt/missing image: {os.path.basename(p)}")
    if not imgs:
        return None, []
    inputs = clip_processor(images=imgs, return_tensors="pt").to(device)
    with torch.no_grad():
        embs = clip_model.get_image_features(**inputs)
    embs = torch.nn.functional.normalize(embs, p=2, dim=1)
    return embs.cpu().numpy(), valid_paths

# ------------------- #
BATCH_SIZE = 1000
total_batches = (len(train) + BATCH_SIZE - 1) // BATCH_SIZE

# ------------------- #
# Embedding loop with skipping existing
skipped_ids = []

for batch_num in range(total_batches):
    emb_file = os.path.join(EMB_DIR, f'image_embeddings_batch{batch_num}.npz')

    # ‚úÖ Skip batch if embeddings already exist
    if os.path.exists(emb_file):
        print(f"Batch {batch_num} embeddings already exist ‚Äî skipping.")
        continue

    start = batch_num * BATCH_SIZE
    end = min((batch_num + 1) * BATCH_SIZE, len(train))
    batch = train.iloc[start:end]

    img_paths = [os.path.join(IMG_DIR, f"{sid}.jpg") for sid in batch["sample_id"]]
    ids = batch["sample_id"].tolist()

    embeddings = []
    valid_ids = []

    for i in tqdm(range(0, len(img_paths), BATCH_SIZE_GPU), desc=f"Embedding batch {batch_num}"):
        emb_batch, valid_paths = get_embeddings_batch(img_paths[i:i+BATCH_SIZE_GPU])
        if emb_batch is not None:
            embeddings.append(emb_batch)
            valid_ids.extend([os.path.splitext(os.path.basename(p))[0] for p in valid_paths])

    # Handle missing images
    skipped_in_batch = set(ids) - set(valid_ids)
    skipped_ids.extend(skipped_in_batch)

    if embeddings:
        embeddings = np.vstack(embeddings)
        np.savez_compressed(
            emb_file,
            ids=np.array(valid_ids), emb=embeddings
        )

    # Update progress
    progress["embedded_batches"].append(batch_num)
    with open(LOG_FILE, "w") as f:
        json.dump(progress, f)

print("‚úÖ All possible embeddings generated.")
if skipped_ids:
    print(f"‚ö†Ô∏è Skipped {len(skipped_ids)} missing/corrupt images.")

"""Step 3: Training Loop with Checkpoints"""

# ============================================================
# üîß Imports
# ============================================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
import numpy as np
import os, json
from tqdm import tqdm

# ============================================================
# ‚öôÔ∏è CONFIG & SETUP
# ============================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

EPOCHS = 20
BATCH_SIZE = 128
LR = 1e-4
VAL_SPLIT = 0.1
CLIP_NORM = 1.0
EARLY_STOP_PATIENCE = 3

# ============================================================
# üß© DATASET CLASS
# ============================================================
class ImageDataset(Dataset):
    def __init__(self, emb_files, prices):
        self.embs = []
        self.ids = []

        # Ensure consistent index type
        prices.index = prices.index.astype(int)

        for f in emb_files:
            try:
                data = np.load(f, allow_pickle=True)
                emb = data["emb"]
                emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)
                ids = [int(i) for i in data["ids"]]
                self.embs.append(emb)
                self.ids.extend(ids)
            except Exception as e:
                print(f"‚ö†Ô∏è Skipping file {f}: {e}")

        self.embs = np.vstack(self.embs)
        self.ids = np.array(self.ids)

        # Filter only valid IDs
        valid_mask = np.isin(self.ids, prices.index)
        self.embs = self.embs[valid_mask]
        self.ids = self.ids[valid_mask]

        self.prices = prices.loc[self.ids].values.astype(np.float32)

        print(f"‚úÖ Loaded {len(self.embs)} samples from embeddings.")

    def __len__(self):
        return len(self.embs)

    def __getitem__(self, idx):
        x = torch.tensor(self.embs[idx], dtype=torch.float32)
        y = torch.tensor(self.prices[idx], dtype=torch.float32)
        return x, y

# ============================================================
# üìÇ LOAD EMBEDDINGS
# ============================================================
emb_files = sorted([
    os.path.join(EMB_DIR, f) for f in os.listdir(EMB_DIR) if f.endswith(".npz")
])

# Log-transform target
prices_log = np.log1p(train.set_index("sample_id")["price"])

dataset = ImageDataset(emb_files, prices_log)

# ============================================================
# üîÄ TRAIN / VAL SPLIT
# ============================================================
indices = np.arange(len(dataset))
train_idx, val_idx = train_test_split(indices, test_size=VAL_SPLIT, random_state=42)
train_loader = DataLoader(Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(Subset(dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False)

# ============================================================
# üß† MODEL ‚Äî Enhanced with LayerNorm, Deeper Layers, SmoothL1
# ============================================================
class PriceRegressor(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.LayerNorm(512),
            nn.Dropout(0.25),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Dropout(0.25),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.net(x)

model = PriceRegressor().to(device)

# ‚úÖ Improved loss for robustness
criterion = nn.SmoothL1Loss()

# ‚úÖ Optimizer with slight L2 regularization
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)

# ‚úÖ Cosine annealing LR scheduler for smooth convergence
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

# ============================================================
# üíæ CHECKPOINT RESUME (Drive-safe)
# ============================================================
latest_ckpt = max(
    [f for f in os.listdir(MODEL_DIR) if f.endswith('.pt')],
    default=None
)
start_epoch = progress.get("trained_epochs", 0)

if latest_ckpt:
    ckpt_path = os.path.join(MODEL_DIR, latest_ckpt)
    ckpt = torch.load(ckpt_path, map_location=device)

    try:
        if isinstance(ckpt, dict) and "model_state_dict" in ckpt:
            model.load_state_dict(ckpt["model_state_dict"])
            if "optimizer_state_dict" in ckpt:
                optimizer.load_state_dict(ckpt["optimizer_state_dict"])
            start_epoch = ckpt.get("epoch", start_epoch)
            print(f"‚úÖ Resuming from checkpoint at epoch {start_epoch}")
        else:
            model.load_state_dict(ckpt)
            print(f"‚öôÔ∏è Loaded raw checkpoint: {latest_ckpt}")
    except RuntimeError as e:
        print(f"‚ùå Failed to load checkpoint {latest_ckpt}: {e}")
        print("Starting fresh training...")
        start_epoch = 0

# ============================================================
# üöÄ TRAINING LOOP (with resume, early stopping, cosine LR)
# ============================================================
best_val_loss = float("inf")
patience_counter = 0

for epoch in range(start_epoch, EPOCHS):
    # ---- Training ----
    model.train()
    train_loss = 0.0
    for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]"):
        x, y = x.to(device), y.to(device).unsqueeze(1)
        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)
        optimizer.step()
        train_loss += loss.item()

    train_loss /= len(train_loader)

    # ---- Validation ----
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for x, y in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
            x, y = x.to(device), y.to(device).unsqueeze(1)
            y_pred = model(x)
            val_loss += criterion(y_pred, y).item()

    val_loss /= len(val_loader)
    scheduler.step()

    print(f"üìä Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    # ---- Save Checkpoint ----
    ckpt_path = os.path.join(MODEL_DIR, f"model_epoch_{epoch+1}.pt")
    torch.save({
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }, ckpt_path)

    # ---- Update progress ----
    progress["trained_epochs"] = epoch + 1
    progress["last_val_loss"] = val_loss
    with open(LOG_FILE, "w") as f:
        json.dump(progress, f)

    # ---- Early stopping ----
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= EARLY_STOP_PATIENCE:
            print("üõë Early stopping triggered.")
            break

print("‚úÖ Training complete!")

"""SMAPE calculation"""

def smape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))

# ------------------------------
# Validation predictions
# ------------------------------
model.eval()
all_preds = []
all_targets = []

with torch.no_grad():
    for x, y in val_loader:
        x, y = x.to(device), y.to(device).unsqueeze(1)
        y_pred = model(x)
        # exponentiate back from log(1+price)
        y_pred_exp = torch.expm1(y_pred)
        y_exp = torch.expm1(y)
        all_preds.extend(y_pred_exp.cpu().numpy())
        all_targets.extend(y_exp.cpu().numpy())

# ------------------------------
# Compute SMAPE
# ------------------------------
val_smape = smape(all_targets, all_preds)
print(f"Validation SMAPE: {val_smape:.4f}%")

"""# Download Test Images"""

import os
import json
import pandas as pd
from PIL import Image
import requests
from io import BytesIO
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------------- #
# Paths
TEST_CSV = os.path.join(DATA_DIR, 'test.csv')
TEST_IMG_DIR = os.path.join(WORK_DIR, 'test_images')
os.makedirs(TEST_IMG_DIR, exist_ok=True)

# Progress logs
TEST_LOG_FILE = os.path.join(WORK_DIR, 'test_progress.json')
FAILED_IMG_LOG = os.path.join(WORK_DIR, 'failed_test_images.json')

# Load or initialize progress
if os.path.exists(TEST_LOG_FILE):
    with open(TEST_LOG_FILE, 'r') as f:
        test_progress = json.load(f)
else:
    test_progress = {"downloaded_batches": [], "embedded_batches": []}

# Load or initialize failed images log
if os.path.exists(FAILED_IMG_LOG):
    with open(FAILED_IMG_LOG, 'r') as f:
        failed_imgs = json.load(f)
else:
    failed_imgs = []

# ------------------- #
# Load test CSV
test_df = pd.read_csv(TEST_CSV)
BATCH_SIZE = 1000
total_batches = (len(test_df) + BATCH_SIZE - 1) // BATCH_SIZE

# ------------------- #
# Download + compress function
def download_and_compress(row):
    sample_id, url = row.sample_id, row.image_link
    img_path = os.path.join(TEST_IMG_DIR, f"{sample_id}.jpg")

    # Skip if already exists
    if os.path.exists(img_path):
        return True

    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        img = Image.open(BytesIO(r.content)).convert('RGB')
        img = img.resize((224, 224))
        img.save(img_path, format='JPEG', quality=75)
        return True
    except Exception as e:
        failed_imgs.append({"url": url, "save_path": img_path, "error": str(e)})
        return False

# ------------------- #
# Process batches
for batch_num in range(total_batches):
    if batch_num in test_progress["downloaded_batches"]:
        continue

    start = batch_num * BATCH_SIZE
    end = min((batch_num + 1) * BATCH_SIZE, len(test_df))
    batch = test_df.iloc[start:end]

    # Parallel download with ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=16) as executor:
        futures = [executor.submit(download_and_compress, row) for row in batch.itertuples()]
        for _ in tqdm(as_completed(futures), total=len(futures), desc=f"Test Batch {batch_num} download"):
            pass

    # Update progress
    test_progress["downloaded_batches"].append(batch_num)
    with open(TEST_LOG_FILE, 'w') as f:
        json.dump(test_progress, f, indent=2)
    with open(FAILED_IMG_LOG, 'w') as f:
        json.dump(failed_imgs, f, indent=2)

print("‚úÖ All test images processed. Failed downloads logged for retry.")

"""# Test: Generate Embeddings"""

import os
import json
import torch
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from transformers import CLIPProcessor, CLIPModel

# ------------------- #
# Device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Paths
TEST_EMB_DIR = os.path.join(WORK_DIR, 'test_embeddings')
os.makedirs(TEST_EMB_DIR, exist_ok=True)

# Load progress logs
TEST_LOG_FILE = os.path.join(WORK_DIR, 'test_progress.json')
FAILED_IMG_LOG = os.path.join(WORK_DIR, 'failed_test_images.json')

with open(TEST_LOG_FILE, 'r') as f:
    test_progress = json.load(f)

if os.path.exists(FAILED_IMG_LOG):
    with open(FAILED_IMG_LOG, 'r') as f:
        failed_imgs = json.load(f)
else:
    failed_imgs = []

# Load test CSV
test_df = pd.read_csv(TEST_CSV)
BATCH_SIZE = 1000
total_batches = (len(test_df) + BATCH_SIZE - 1) // BATCH_SIZE

# ------------------- #
# Load CLIP
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# GPU/CPU batch size auto
def get_auto_batch_size(max_mem_gb=12, approx_image_mb=0.05):
    if device == "cpu":
        return 8  # safe small batch for CPU
    free_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    batch_size = int(min(max_mem_gb / approx_image_mb, free_mem_gb / approx_image_mb))
    return max(1, min(batch_size, 128))

BATCH_SIZE_GPU = get_auto_batch_size()
print(f"‚úÖ Batch size for embeddings: {BATCH_SIZE_GPU}")

# ------------------- #
# Image loading with try/except
def load_image_safe(path):
    try:
        img = Image.open(path).convert("RGB")
        return img, path
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to open image {os.path.basename(path)}: {e}")
        failed_imgs.append({"save_path": path})
        return None, path

# ------------------- #
# Generate embeddings for a batch
def get_embeddings_batch(img_paths):
    imgs, valid_paths = [], []
    # Threaded image loading
    with ThreadPoolExecutor(max_workers=16) as executor:
        results = list(executor.map(load_image_safe, img_paths))
    for img, path in results:
        if img is not None:
            imgs.append(img)
            valid_paths.append(path)
    if not imgs:
        return None, []
    inputs = clip_processor(images=imgs, return_tensors="pt").to(device)
    with torch.no_grad():
        embs = clip_model.get_image_features(**inputs)
    embs = torch.nn.functional.normalize(embs, p=2, dim=1)
    return embs.cpu().numpy(), valid_paths

# ------------------- #
# Generate embeddings for all batches
for batch_num in range(total_batches):
    emb_file = os.path.join(TEST_EMB_DIR, f'test_embeddings_batch{batch_num}.npz')
    if os.path.exists(emb_file):
        print(f"Batch {batch_num} already exists, skipping...")
        if batch_num not in test_progress["embedded_batches"]:
            test_progress["embedded_batches"].append(batch_num)
        continue

    if batch_num not in test_progress["downloaded_batches"]:
        print(f"Batch {batch_num} not downloaded yet, skipping...")
        continue

    start = batch_num * BATCH_SIZE
    end = min((batch_num + 1) * BATCH_SIZE, len(test_df))
    batch = test_df.iloc[start:end]

    # Prepare image paths
    img_paths = []
    ids = []
    for row in batch.itertuples():
        img_path = os.path.join(TEST_IMG_DIR, f"{row.sample_id}.jpg")
        if os.path.exists(img_path) and not any(f["save_path"].endswith(f"{row.sample_id}.jpg") for f in failed_imgs):
            img_paths.append(img_path)
            ids.append(row.sample_id)

    if not img_paths:
        print(f"Batch {batch_num} has no valid images, skipping embedding...")
        continue

    # Generate embeddings in GPU batches
    embeddings_list = []
    valid_ids = []
    for i in range(0, len(img_paths), BATCH_SIZE_GPU):
        emb_batch, valid_paths = get_embeddings_batch(img_paths[i:i+BATCH_SIZE_GPU])
        if emb_batch is not None:
            embeddings_list.append(emb_batch)
            valid_ids.extend([int(os.path.splitext(os.path.basename(p))[0]) for p in valid_paths])

    if embeddings_list:
        embeddings = np.vstack(embeddings_list)
        np.savez_compressed(emb_file, ids=np.array(valid_ids), emb=embeddings)

    # Update progress
    test_progress["embedded_batches"].append(batch_num)
    with open(TEST_LOG_FILE, 'w') as f:
        json.dump(test_progress, f, indent=2)
    with open(FAILED_IMG_LOG, 'w') as f:
        json.dump(failed_imgs, f, indent=2)

print("‚úÖ All test embeddings generated (skipped missing/failed images).")

"""# Prediction"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import os
import json
from tqdm import tqdm

# ------------------- #
# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üß† Using device: {device}")

# ------------------- #
# Paths
PRED_LOG = os.path.join(WORK_DIR, "predict_progress.json")
SUB_PART_DIR = os.path.join(WORK_DIR, "submission_parts")
FINAL_SUB_PATH = os.path.join(WORK_DIR, "submission.csv")
os.makedirs(SUB_PART_DIR, exist_ok=True)

# ------------------- #
# Resume progress
if os.path.exists(PRED_LOG):
    with open(PRED_LOG, "r") as f:
        predict_progress = json.load(f)
else:
    predict_progress = {"completed_batches": []}

# ------------------- #
# Dataset class
class TestDataset(Dataset):
    def __init__(self, emb_files):
        self.embs, self.ids = [], []
        for f in sorted(emb_files):
            try:
                data = np.load(f)
                if "emb" not in data or "ids" not in data:
                    print(f"‚ö†Ô∏è Skipping invalid embedding file: {f}")
                    continue
                self.embs.append(data["emb"])
                self.ids.extend(data["ids"])
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {f}: {e}")
        if len(self.embs) == 0:
            raise ValueError("No valid embeddings found!")
        self.embs = np.vstack(self.embs)
        print(f"‚úÖ Loaded {len(self.ids)} test samples from {len(emb_files)} embedding files")

    def __len__(self):
        return len(self.embs)

    def __getitem__(self, idx):
        return torch.tensor(self.embs[idx], dtype=torch.float32), self.ids[idx]

# ------------------- #
# Load test embeddings
test_emb_files = sorted([
    os.path.join(TEST_EMB_DIR, f)
    for f in os.listdir(TEST_EMB_DIR)
    if f.endswith(".npz")
])

if not test_emb_files:
    raise ValueError("No test embeddings found. Run embedding generation first!")

# Split into per-file batches for safe resume
print(f"üì¶ Found {len(test_emb_files)} embedding batches")
batch_files = [
    f for i, f in enumerate(test_emb_files)
    if i not in predict_progress["completed_batches"]
]

if not batch_files:
    print("‚úÖ All embedding batches already predicted! Merging existing results...")
else:
    print(f"üß© Remaining batches to predict: {len(batch_files)}")

# ------------------- #
# Load trained model
model_ckpt = max([f for f in os.listdir(MODEL_DIR) if f.endswith(".pt")], default=None)
if model_ckpt is None:
    raise ValueError("No trained model checkpoint found!")

ckpt_path = os.path.join(MODEL_DIR, model_ckpt)
ckpt = torch.load(ckpt_path, map_location=device)
if isinstance(ckpt, dict) and "model_state_dict" in ckpt:
    model.load_state_dict(ckpt["model_state_dict"])
else:
    model.load_state_dict(ckpt)
model.to(device)
model.eval()
print(f"‚úÖ Loaded model checkpoint: {model_ckpt}")

# ------------------- #
# Prediction function
def predict_embeddings_file(file_path, batch_idx):
    data = np.load(file_path)
    if "emb" not in data or "ids" not in data:
        print(f"‚ö†Ô∏è Invalid embedding file skipped: {file_path}")
        return

    embs = torch.tensor(data["emb"], dtype=torch.float32)
    ids = data["ids"]
    preds, ids_out = [], []

    loader = DataLoader(
        torch.utils.data.TensorDataset(embs, torch.arange(len(ids))),
        batch_size=256,
        shuffle=False,
    )

    with torch.no_grad():
        for x, idxs in tqdm(loader, desc=f"Predicting batch {batch_idx}", leave=False):
            x = x.to(device)
            y_pred = model(x).cpu().numpy()
            preds.extend(np.expm1(y_pred.flatten()))  # revert log1p
            ids_out.extend(ids[idxs])

    # Save partial result
    out_df = pd.DataFrame({"sample_id": ids_out, "price": preds})
    out_path = os.path.join(SUB_PART_DIR, f"submission_part_{batch_idx}.csv")
    out_df.to_csv(out_path, index=False)

    # Log progress
    predict_progress["completed_batches"].append(batch_idx)
    with open(PRED_LOG, "w") as f:
        json.dump(predict_progress, f, indent=2)

    print(f"‚úÖ Saved predictions for batch {batch_idx} -> {out_path}")

# ------------------- #
# Run prediction per embedding file
for i, fpath in enumerate(batch_files):
    batch_idx = test_emb_files.index(fpath)
    predict_embeddings_file(fpath, batch_idx)

# ------------------- #
# Merge all partial submissions
print("üß© Merging all partial CSVs...")
merged = pd.concat([
    pd.read_csv(os.path.join(SUB_PART_DIR, f))
    for f in sorted(os.listdir(SUB_PART_DIR)) if f.endswith(".csv")
])
merged = merged.sort_values("sample_id")
merged.to_csv(FINAL_SUB_PATH, index=False)
print(f"üéØ Final submission saved to: {FINAL_SUB_PATH}")

