{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Validation Predictions & Weight Tuning"],"metadata":{"id":"Zuosyun6vGG3"}},{"cell_type":"code","execution_count":15,"metadata":{"id":"Xhs2Um3FuF0X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"129a689d-f63d-46b1-fbdb-c92cd1f7af5b","executionInfo":{"status":"ok","timestamp":1760378628425,"user_tz":-330,"elapsed":30183,"user":{"displayName":"Sumit","userId":"04364667860693544795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","‚úÖ X_train_text shape: (75000, 2390)\n","‚úÖ y_train shape: (75000,)\n"]},{"output_type":"stream","name":"stderr","text":["üñºÔ∏è Loading validation images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7500/7500 [00:18<00:00, 414.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Loaded image model checkpoint: /content/drive/MyDrive/Amazon_ML_Challenge/student_resource/models/model_epoch_20.pt\n","üîπ Loading cached image validation predictions...\n","‚úÖ Best Weight ‚Üí Text: 1.00, Image: 0.00\n","üìä SMAPE: 21.0106\n"]}],"source":["# ============================================================\n","# üì¶ ENSEMBLE VALIDATION (MULTITHREADED, RESUMABLE & MEMORY-SAFE)\n","# ============================================================\n","import os\n","import sys\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from transformers import CLIPProcessor, CLIPModel\n","import joblib\n","from concurrent.futures import ThreadPoolExecutor\n","from PIL import Image\n","from scipy.sparse import csr_matrix\n","\n","# ============================================================\n","# ‚öôÔ∏è CONFIG / PATHS\n","# ============================================================\n","WORK_DIR = '/content/drive/MyDrive/Amazon_ML_Challenge/student_resource'\n","TEXT_CACHE_DIR = os.path.join(WORK_DIR, \"cache\")\n","IMAGE_DIR = os.path.join(WORK_DIR, \"images\")\n","TRAIN_CSV = os.path.join(WORK_DIR, \"dataset\", \"train.csv\")\n","VAL_PROGRESS_FILE = os.path.join(WORK_DIR, \"ensemble_val_progress.json\")\n","IMAGE_VAL_PRED_FILE = os.path.join(WORK_DIR, \"image_val_preds.npy\")\n","MODEL_DIR = os.path.join(WORK_DIR, \"models\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# ============================================================\n","# 1Ô∏è‚É£ LOAD TEXT DATA & MODEL\n","# ============================================================\n","npz_data = np.load(os.path.join(TEXT_CACHE_DIR, \"X_train_step3.npz\"), allow_pickle=True)\n","X_train_text = csr_matrix(\n","    (npz_data['data'], npz_data['indices'], npz_data['indptr']),\n","    shape=tuple(npz_data['shape'])\n",")\n","y_train = np.load(os.path.join(TEXT_CACHE_DIR, \"y_train_step3.npy\"))\n","\n","assert X_train_text.shape[0] == len(y_train)\n","print(\"‚úÖ X_train_text shape:\", X_train_text.shape)\n","print(\"‚úÖ y_train shape:\", y_train.shape)\n","\n","text_model_path = os.path.join(TEXT_CACHE_DIR, \"best_model_lightgbm.pkl\")\n","text_model = joblib.load(text_model_path)\n","\n","X_tr, X_val_text, y_tr, y_val = train_test_split(\n","    X_train_text, y_train, test_size=0.1, random_state=42\n",")\n","text_val_preds = text_model.predict(X_val_text)\n","\n","# ============================================================\n","# 2Ô∏è‚É£ LOAD & PREPROCESS VALIDATION IMAGES (BATCHED, RESUMABLE)\n","# ============================================================\n","train_df = pd.read_csv(TRAIN_CSV)\n","val_ids = train_df.sample(frac=0.1, random_state=42)[\"sample_id\"].values\n","val_img_paths = [os.path.join(IMAGE_DIR, f\"{sid}.jpg\") for sid in val_ids]\n","\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","clip_model.eval()\n","\n","def load_image(path):\n","    try:\n","        img = Image.open(path).convert(\"RGB\")\n","        return img\n","    except:\n","        return None\n","\n","# Multithreaded image loading\n","with ThreadPoolExecutor(max_workers=8) as executor:\n","    val_images = list(tqdm(executor.map(load_image, val_img_paths),\n","                           total=len(val_img_paths),\n","                           desc=\"üñºÔ∏è Loading validation images\"))\n","\n","# Filter out failed images\n","valid_idx = [i for i, img in enumerate(val_images) if img is not None]\n","val_images = [val_images[i] for i in valid_idx]\n","val_ids = val_ids[valid_idx]\n","\n","# ============================================================\n","# 3Ô∏è‚É£ LOAD IMAGE MODEL (PriceRegressor)\n","# ============================================================\n","sys.path.append(WORK_DIR)\n","from price_regressor import PriceRegressor\n","\n","model_ckpts = [f for f in os.listdir(MODEL_DIR) if f.endswith(\".pt\")]\n","if not model_ckpts:\n","    raise FileNotFoundError(f\"No .pt model checkpoint found in {MODEL_DIR}\")\n","\n","latest_ckpt = max(model_ckpts, key=lambda f: os.path.getmtime(os.path.join(MODEL_DIR, f)))\n","ckpt_path = os.path.join(MODEL_DIR, latest_ckpt)\n","\n","image_model = PriceRegressor().to(device)\n","ckpt = torch.load(ckpt_path, map_location=device)\n","if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n","    image_model.load_state_dict(ckpt[\"model_state_dict\"])\n","else:\n","    image_model.load_state_dict(ckpt)\n","image_model.eval()\n","print(\"‚úÖ Loaded image model checkpoint:\", ckpt_path)\n","\n","# ============================================================\n","# 4Ô∏è‚É£ LOAD OR INITIALIZE PROGRESS\n","# ============================================================\n","if os.path.exists(VAL_PROGRESS_FILE):\n","    with open(VAL_PROGRESS_FILE, \"r\") as f:\n","        progress = json.load(f)\n","else:\n","    progress = {\"last_index\": 0, \"image_val_done\": False}\n","\n","if os.path.exists(IMAGE_VAL_PRED_FILE):\n","    image_val_preds = list(np.load(IMAGE_VAL_PRED_FILE))\n","else:\n","    image_val_preds = []\n","\n","# ============================================================\n","# 5Ô∏è‚É£ GENERATE IMAGE VALIDATION PREDICTIONS (RESUMABLE)\n","# ============================================================\n","BATCH_SIZE_PROC = 64\n","start_idx = progress.get(\"last_index\", 0)\n","\n","if not progress.get(\"image_val_done\", False):\n","    print(f\"üîπ Resuming from index {start_idx}...\")\n","\n","    with torch.no_grad():\n","        for i in tqdm(range(start_idx, len(val_images), BATCH_SIZE_PROC), desc=\"üéØ Image Val Predictions\"):\n","            batch_imgs = val_images[i:i+BATCH_SIZE_PROC]\n","            inputs = processor(images=batch_imgs, return_tensors=\"pt\", padding=True)\n","            batch_pixels = inputs[\"pixel_values\"].to(device)\n","\n","            # --- Extract CLIP embeddings first ---\n","            batch_embeds = clip_model.get_image_features(batch_pixels)\n","\n","            # Feed embeddings to PriceRegressor\n","            preds = image_model(batch_embeds).cpu().numpy().flatten()\n","            image_val_preds.extend(preds)\n","\n","            # Save partial predictions & progress\n","            np.save(IMAGE_VAL_PRED_FILE, np.array(image_val_preds))\n","            progress[\"last_index\"] = i + BATCH_SIZE_PROC\n","            with open(VAL_PROGRESS_FILE, \"w\") as f:\n","                json.dump(progress, f)\n","\n","    progress[\"image_val_done\"] = True\n","    with open(VAL_PROGRESS_FILE, \"w\") as f:\n","        json.dump(progress, f)\n","else:\n","    print(\"üîπ Loading cached image validation predictions...\")\n","    image_val_preds = np.load(IMAGE_VAL_PRED_FILE)\n","\n","# ============================================================\n","# 6Ô∏è‚É£ ENSEMBLE WEIGHT TUNING (SMAPE)\n","# ============================================================\n","def smape(y_true, y_pred):\n","    return 100/len(y_true) * np.sum(\n","        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)\n","    )\n","\n","# Convert to numpy array\n","image_val_preds = np.array(image_val_preds)\n","\n","# Now ensemble tuning\n","best_smape = float(\"inf\")\n","best_weight = 0.5\n","n_val = min(len(text_val_preds), len(image_val_preds))\n","\n","for w in np.arange(0.0, 1.01, 0.05):\n","    val_preds = w * text_val_preds[:n_val] + (1 - w) * image_val_preds[:n_val]\n","    score = smape(y_val[:n_val], val_preds)\n","    if score < best_smape:\n","        best_smape = score\n","        best_weight = w\n","\n","print(f\"‚úÖ Best Weight ‚Üí Text: {best_weight:.2f}, Image: {1-best_weight:.2f}\")\n","print(f\"üìä SMAPE: {best_smape:.4f}\")\n","\n","\n","with open(os.path.join(WORK_DIR, \"best_ensemble_weight.json\"), \"w\") as f:\n","    json.dump({\"text\": best_weight, \"image\": 1-best_weight}, f)\n"]},{"cell_type":"markdown","source":["# Final Test Predictions & Submission"],"metadata":{"id":"Zyi47omNu-4Z"}},{"cell_type":"code","source":["# Notebook 2: final_submission_multithread.ipynb\n","import os\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import joblib\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# ------------------- #\n","# CONFIG / PATHS\n","WORK_DIR = '/content/drive/MyDrive/Amazon_ML_Challenge/student_resource'\n","TEXT_CACHE_DIR = os.path.join(WORK_DIR, \"cache\")\n","IMAGE_DIR = os.path.join(WORK_DIR, \"images\")\n","TRAIN_CSV = os.path.join(WORK_DIR, \"dataset\", \"train.csv\")\n","TEST_CSV = os.path.join(WORK_DIR, \"dataset\", \"test.csv\")\n","SUBMISSION_PATH = os.path.join(WORK_DIR, \"submission_final.csv\")\n","WEIGHT_FILE = os.path.join(WORK_DIR, \"best_ensemble_weight.json\")\n","IMAGE_TEST_PRED_FILE = os.path.join(WORK_DIR, \"image_test_preds.npy\")\n","PROGRESS_FILE = os.path.join(WORK_DIR, \"ensemble_test_progress.json\")\n","VAL_PROGRESS_FILE = os.path.join(WORK_DIR, \"ensemble_val_progress.json\")\n","IMAGE_VAL_PRED_FILE = os.path.join(WORK_DIR, \"image_val_preds.npy\")\n","\n","\n","# ------------------- #\n","# DEVICE\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# ------------------- #\n","# 1Ô∏è‚É£ Load best ensemble weight\n","with open(WEIGHT_FILE, \"r\") as f:\n","    weight_data = json.load(f)\n","TEXT_WEIGHT = weight_data[\"text\"]\n","IMAGE_WEIGHT = weight_data[\"image\"]\n","\n","# ------------------- #\n","# 2Ô∏è‚É£ Load text test predictions\n","npz_data = np.load(os.path.join(TEXT_CACHE_DIR, \"X_test_step3.npz\"), allow_pickle=True)\n","X_test_text = csr_matrix((npz_data['data'], npz_data['indices'], npz_data['indptr']),\n","                         shape=tuple(npz_data['shape']))\n","text_model_path = os.path.join(TEXT_CACHE_DIR, \"best_model_lightgbm.pkl\")\n","text_model = joblib.load(text_model_path)\n","text_test_preds = text_model.predict(X_test_text)\n","\n","# ------------------- #\n","# 3Ô∏è‚É£ Load image test embeddings using multithreading\n","def load_npz_file(path):\n","    try:\n","        data = np.load(path)\n","        return data[\"emb\"], data[\"ids\"]\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Failed to load {path}: {e}\")\n","        return None, None\n","\n","IMAGE_EMB_DIR = os.path.join(WORK_DIR, \"test_embeddings\")  # adjust path\n","image_emb_files = sorted([os.path.join(IMAGE_EMB_DIR, f) for f in os.listdir(IMAGE_EMB_DIR) if f.endswith(\".npz\")])\n","\n","all_embs, all_ids = [], []\n","\n","with ThreadPoolExecutor(max_workers=8) as executor:\n","    for emb, ids in tqdm(executor.map(load_npz_file, image_emb_files), total=len(image_emb_files), desc=\"Loading image embeddings\"):\n","        if emb is not None and ids is not None:\n","            all_embs.append(emb)\n","            all_ids.extend(ids)\n","\n","image_embs = np.vstack(all_embs)\n","\n","# Dataset & loader\n","class ImageEmbDataset(Dataset):\n","    def __init__(self, embs, ids):\n","        self.embs = torch.tensor(embs, dtype=torch.float32)\n","        self.ids = ids\n","\n","    def __len__(self):\n","        return len(self.embs)\n","\n","    def __getitem__(self, idx):\n","        return self.embs[idx], self.ids[idx]\n","\n","image_dataset = ImageEmbDataset(image_embs, all_ids)\n","image_loader = DataLoader(image_dataset, batch_size=128, shuffle=False)\n","\n","# ------------------- #\n","# 4Ô∏è‚É£ Load image model\n","from price_regressor import PriceRegressor\n","\n","\n","image_model_ckpt = max([f for f in os.listdir(os.path.join(WORK_DIR, \"models\")) if f.endswith(\".pt\")])\n","ckpt = torch.load(os.path.join(WORK_DIR, \"models\", image_model_ckpt), map_location=device)\n","\n","image_model = PriceRegressor().to(device)\n","if \"model_state_dict\" in ckpt:\n","    image_model.load_state_dict(ckpt[\"model_state_dict\"])\n","else:\n","    image_model.load_state_dict(ckpt)\n","image_model.eval()\n","\n","# ------------------- #\n","# 5Ô∏è‚É£ Resume or generate image test predictions (resumable & aligned)\n","if os.path.exists(PROGRESS_FILE):\n","    with open(PROGRESS_FILE, \"r\") as f:\n","        progress = json.load(f)\n","else:\n","    progress = {\"image_test_done\": False}\n","\n","# Load previously saved predictions if exists\n","image_test_preds_dict = {}\n","if os.path.exists(IMAGE_TEST_PRED_FILE):\n","    try:\n","        loaded = np.load(IMAGE_TEST_PRED_FILE, allow_pickle=True)\n","        # Handle old format (plain array) or new format (dict)\n","        if isinstance(loaded, np.ndarray) and loaded.size == 1:\n","            # old array saved as single element\n","            image_test_preds_dict = loaded.item()\n","        elif isinstance(loaded, np.ndarray) and loaded.dtype == object:\n","            image_test_preds_dict = loaded.item()\n","        else:\n","            # fallback: assume array of predictions without IDs\n","            print(\"‚ö†Ô∏è Warning: IMAGE_TEST_PRED_FILE is not a dict; predictions will be overwritten.\")\n","            image_test_preds_dict = {}\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Failed to load {IMAGE_TEST_PRED_FILE}: {e}\")\n","        image_test_preds_dict = {}\n","\n","if not progress.get(\"image_test_done\", False):\n","    print(\"üîπ Generating image test predictions...\")\n","\n","    with torch.no_grad():\n","        for x, ids in tqdm(image_loader, desc=\"Image Test Predictions\"):\n","            x = x.to(device)\n","            y_pred = image_model(x).cpu().numpy().flatten()\n","            for sid, pred in zip(ids, y_pred):\n","                image_test_preds_dict[int(sid)] = float(pred)\n","\n","            # Save progress every batch\n","            np.save(IMAGE_TEST_PRED_FILE, image_test_preds_dict)\n","\n","    progress[\"image_test_done\"] = True\n","    with open(PROGRESS_FILE, \"w\") as f:\n","        json.dump(progress, f)\n","else:\n","    print(\"üîπ Loading cached image test predictions...\")\n","\n","# ------------------- #\n","# 6Ô∏è‚É£ Weighted ensemble & save submission (aligned by sample_id)\n","test_df = pd.read_csv(TEST_CSV)\n","\n","# Ensure text_test_preds is a series with sample_id index\n","text_preds_series = pd.Series(text_test_preds, index=test_df['sample_id'])\n","\n","# Image predictions aligned by sample_id; fill missing with 0\n","image_preds_series = test_df['sample_id'].map(image_test_preds_dict).fillna(0)\n","\n","# Weighted ensemble\n","final_preds = TEXT_WEIGHT * text_preds_series.values + IMAGE_WEIGHT * image_preds_series.values\n","\n","# Revert log1p if used during training\n","final_preds = np.expm1(final_preds)\n","\n","# Save submission\n","submission = test_df[['sample_id']].copy()\n","submission['price'] = final_preds\n","submission.to_csv(SUBMISSION_PATH, index=False)\n","print(f\"‚úÖ Submission saved at {SUBMISSION_PATH}\")\n"],"metadata":{"id":"fJcXG-0Ku7Xv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760379488150,"user_tz":-330,"elapsed":43539,"user":{"displayName":"Sumit","userId":"04364667860693544795"}},"outputId":"83107ee2-783c-48a6-9248-0fce3b9b2968"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Loading image embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:02<00:00, 36.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["‚ö†Ô∏è Warning: IMAGE_TEST_PRED_FILE is not a dict; predictions will be overwritten.\n","üîπ Loading cached image test predictions...\n","‚úÖ Submission saved at /content/drive/MyDrive/Amazon_ML_Challenge/student_resource/submission_final.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NyQUZt7ywEUu"},"execution_count":null,"outputs":[]}]}